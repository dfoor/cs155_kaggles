    
def get_error(G, Y):
    error=0
    for i in xrange(len(G)):
        if G[i] != Y[i]:
            error += 1
    return 1.0 * error / len(G)


def demo(train,test):

    import os
    import sys
    import csv
    import numpy as np
    import matplotlib.pyplot as plt
    from sklearn import linear_model
    from sklearn import svm
    from sklearn import tree
    from sklearn.cross_validation import cross_val_score
    from sklearn.tree import DecisionTreeRegressor
    import pprint

    with open(train, 'rb') as csvfile:
        data=np.array(list(csv.reader(csvfile))).astype(np.float32)
    with open(test, 'rb') as csvfile:
        data2=np.array(list(csv.reader(csvfile))).astype(np.float32)
    
    num_dim= len(data[0])-2

    x_train = data[::, 1:-1]
    y_train = data[::, -1]
    x_test = data2[::, 1:]
    print y_train
    print 'data read'
    

    #SVM implementaiton

    test_errors =[]
    train_errors =[]
    
    for C in [.0001]:
        X1 = x_train
        results_to_num = y_train

        #clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
        #intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',
        #random_state=None, tol=C, verbose=0)
        
        #clf = linear_model.Lasso(alpha=C, copy_X=True, fit_intercept=True, max_iter=1000,
        #   normalize=False, positive=False, precompute='auto', tol=0.0001,
        #   warm_start=False)
        
        #clf = tree.DecisionTreeClassifier() #no winner even with 83 percent

        regressor = tree.DecisionTreeRegressor(random_state=0)
        clf = cross_val_score(regressor, X1, results_to_num, cv=10)

        


        x_sparse_train = tree.transform(X1) #extract out the most important features (using mean)

        
        #clf = svm.SVC(C=1.0, #rbf model
        #          cache_size=200,
        #          class_weight=None,
        #          coef0=0.0,
        #          degree=C,
        #          gamma=0.0,
        #          kernel='rbf',
        #          max_iter=-1,
        #          probability=False,
        #          random_state=None,
        #          shrinking=True,
        #          tol=0.001,
        #          verbose=False)

        #current winner
        #run svm over the sparse set
        clf = svm.SVC(
                kernel='poly',
                C=1,
                degree=3,
                shrinking=False,
                gamma=1,
                coef0=1)

        print 'fitting data 1'
        clf.fit(X1, results_to_num)
        print 'fitting data 2'
        clf.fit(x_sparse_train, results_to_num)

        #used for linear_model.Lasso
        #f = open("coefs_coef_%s" % C, "w")
        #f.write('%s' % clf.coef_)
        #print('wrote coef file to coefs_coef_%s' % C)
        #f.close()
        #print(clf.intercept_)

        #used for poly svm
        #print 'print(len(clf.dual_coef_))'
        #print(len(clf.dual_coef_))
        #print 'print(len(clf.support_vectors_))'
        #print(len(clf.support_vectors_))


        for j in [x_sparse_train, x_train]:
            print 'predicting results'
            G_train = clf.predict(j)
            G_test = clf.predict(x_test)

            train_error = get_error(G_train, y_train)
            print 'train_error ', train_error

            tally = 0
            if j==x_sparse_train: f = open("output_decision_tree_sparse.csv", "w")
            if j==x_train:        f = open("output_decision_tree_full.csv", "w")
            f.write('Id,Prediction\n')
            for i in xrange(len(x_test)):
                if G_test[i]==0: tally+=1
                f.write('%s,'% (i+1))
                f.write('%s\n'% G_test[i])
            f.close()
            print 'classified ', tally/float(len(x_test)), '% as non-recession for C=', C

    
if __name__ == '__main__':
    print('k3.csv')
    demo('kaggle_train_wc.csv','kaggle_test_wc.csv')
