#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\headheight 1in
\headsep 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation skip
\defskip smallskip
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
David Foor
\end_layout

\begin_layout Standard
CS 156A Kaggle Competition 
\end_layout

\begin_layout Standard
Due 2 26, 2014
\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Section*
1 Overview
\end_layout

\begin_layout Standard
Below I outline the various tests I ran in the competition, each test has
 it's own approach and thus its own overview, data, learning, and model
 selection components.
\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Section
No Learning
\end_layout

\begin_layout Subsubsection*
1 Overview
\end_layout

\begin_layout Standard
The first submissions I made included a random output selection, which returned
 49% correctness (surprise surprise), and an all zeros submission, which
 garnered an 83% accuracy.
 This test gave me information about the distribution of recession to no-recessi
on reports.
 Thus off the bat, and assuming that the hidden test set is of the same
 distribution (we were told that it is), I should expect for a good data
 train, that the learning algorithm classifies 83% of the data into the
 no-recession catagory.
\end_layout

\begin_layout Subsubsection*
2 Data Manipulation
\end_layout

\begin_layout Subsubsection*
3 Learning Algorithm
\end_layout

\begin_layout Subsubsection*
4 Model Selection
\end_layout

\begin_layout Standard
\begin_inset CommandInset line
LatexCommand rule
offset "0.5ex"
width "100col%"
height "1pt"

\end_inset


\end_layout

\begin_layout Standard
SVM implementation using polynomials of varying degrees:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png

\end_inset


\end_layout

\begin_layout Standard
Submitted deg 2, received 89%:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted5.png

\end_inset


\end_layout

\begin_layout Standard
SVM implementation using radial basis functions of varying degrees (2,3,5
 deg):
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted3.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png

\end_inset


\end_layout

\begin_layout Standard
This had poor results, leaving most of the points classified as non-recession.
\end_layout

\begin_layout Standard
Next tried was Linear Support Vector Classification of varying tolerances
 (.0001,.001,.01) and it had the various probabilities:
\end_layout

\begin_layout Standard
Decided to submit both 0.001 and 0.001:
\end_layout

\begin_layout Standard
.0001:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted7.png

\end_inset


\end_layout

\begin_layout Standard
0.001 only got 86% out of sample error.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted6.png

\end_inset


\end_layout

\begin_layout Standard
Try: use lasso to make the weights sparse, then use SVM rbf with kernel
 trick, lasso has convergence issues on this run:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted8.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted9.png

\end_inset


\end_layout

\begin_layout Standard
Tryed decision tree and got a very promsing result:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted10.png

\end_inset


\end_layout

\begin_layout Standard
it classified 83.2 % as belonging to the non-resession, which, is the same
 distribution as the test set.
 
\end_layout

\begin_layout Standard
Tried random forest using cross validation and selected the model with a
 minimum group size of 5:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted11.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted12.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted13.png

\end_inset


\end_layout

\begin_layout Standard
Woo hoo.
 90%
\end_layout

\begin_layout Standard
Got smart and started to use crossvalidation.
 I tried multiple SVM impelentaitons (radial basis functions, stochastic
 gradient descent on linear model, polynomial models), and found that tuning
 C to a relaxed .0001 lead to the best performance.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted15.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted16.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted14.png

\end_inset


\end_layout

\begin_layout Standard
Tried random forest classifier in CV:
\end_layout

\begin_layout Standard
Best was around 88%.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted17.png

\end_inset


\end_layout

\begin_layout Standard
Tried AdaBoostClassiifer:
\end_layout

\begin_layout Standard
Best I could get was around 89%:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted18.png

\end_inset


\end_layout

\begin_layout Standard
AdaBoostRegressor:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted19.png

\end_inset


\end_layout

\begin_layout Standard
Tried the various linear_model classes.
 Each that didn't error out are shown with their first-pass CV accuracy.
 It looks like RidgeClassifier would be a good place to look:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted20.png

\end_inset


\end_layout

\begin_layout Standard
Couldn't get Ridge any better.
\end_layout

\begin_layout Standard
Tried neural nets;
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted22.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted21.png

\end_inset


\end_layout

\begin_layout Standard
Used grid search to explore the various parameter spaces.
 Looks like rbf gets possibly better than the current victor--the linear
 C=.0001 model.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted23.png

\end_inset


\end_layout

\begin_layout Standard
clearly, nope--and it matches the CV score:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted24.png

\end_inset


\end_layout

\begin_layout Standard
heh heh.
\end_layout

\begin_layout Standard
I took the results from the test set and included them in my training set.
 Becuase my regularizer is pretty loose, and because I scored better than
 1/2 on the test set, i was able to increase the score--i think.
 I will try this recursivly until something theoretical breaks it.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted25.png

\end_inset


\end_layout

\begin_layout Standard
Second time I tried this, it broke:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted26.png

\end_inset


\end_layout

\begin_layout Standard
Okay--using adaboost, I got a better result and still had training error,
 so I'll try more classifiers:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted27.png

\end_inset


\end_layout

\begin_layout Standard
GradientBoostClassifier returned even better error at 93% and with a tuned
 max depth of 3:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted28.png

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted29.png

\end_inset


\end_layout

\begin_layout Section*
5 Conclusion
\end_layout

\end_body
\end_document
